{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexmoed/MasterClass/blob/3D-Pointnet%2B%2B/PointNet_Classification_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pnOKJBB5xk79"
      },
      "outputs": [],
      "source": [
        " # !git clone https://github.com/karol-202/direct-3dgs-segmentation \"/content/drive/MyDrive/3dgs/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8Ax15U0J8pB",
        "outputId": "c169a0c5-832f-44c0-b9cd-b0c29b4e83d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#change the directory to the correct spot on google drive\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train a semantic segmentation model for 3D Gaussian Splats\n",
        "# Modified from :\n",
        "# Yanx27 (2019). PointNet_Pointnet2_pytorch [online].\n",
        "# [Accessed 2024]. Available from: \"https://github.com/yanx27/Pointnet_Pointnet2_pytorch\"\n",
        "# Based on research from:\n",
        "# Jurski, K. (2024). Semantic 3D segmentation of 3D Gaussian Splats: Assessing existing\n",
        "# point cloud segmentation techniques on semantic segmentation of synthetic 3D Gaussian\n",
        "# Splats scenes. Bachelor's Thesis, Delft University of Technology.\n",
        "# Extended implementation: \"https://github.com/karol-202/direct-3dgs-segmentation\""
      ],
      "metadata": {
        "id": "wNHhvtbP9qwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PointNet ++ for Gaussian splats"
      ],
      "metadata": {
        "id": "w7gebjNX7XYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing depencencies"
      ],
      "metadata": {
        "id": "EJCG4e5x7cQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciTW75Hh_6_w",
        "outputId": "d8eccffb-b651-481c-e6ef-7f24b4cf255e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting plyfile\n",
            "  Downloading plyfile-1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from plyfile) (2.0.2)\n",
            "Downloading plyfile-1.1-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: plyfile\n",
            "Successfully installed plyfile-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install plyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlodEOZnyQWS",
        "outputId": "e7b9155d-b7f8-4b1c-aa17-dfa2057b88ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[33mWARNING: Skipping pytorch3d as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#Uninstall any versions of pytorch3d\n",
        "!pip uninstall -y torch torchvision torchaudio pytorch3d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egDDaEHnyT9X",
        "outputId": "33277cc5-0525-49b5-ec71-66b99dcd7bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torch-2.6.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (768.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.0/768.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torchvision-0.20.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/torchaudio-2.5.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-triton==3.1.0+cf34004b8a (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.1.0%2Bcf34004b8a-cp311-cp311-linux_x86_64.whl (239.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.7/239.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: pytorch-triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 pytorch-triton-3.1.0+cf34004b8a torch-2.6.0.dev20241112+cu121 torchaudio-2.5.0.dev20241112+cu121 torchvision-0.20.0.dev20241112+cu121\n"
          ]
        }
      ],
      "source": [
        "#Run this exact version of torch vision others dont seem to work with pytorch 3d\n",
        "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dSilur4IzHSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478789f5-0cb6-4d6c-df80-0e9b70453d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (3.31.6)\n",
            "Collecting cmake\n",
            "  Downloading cmake-4.0.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Downloading cmake-4.0.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja, cmake\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.31.6\n",
            "    Uninstalling cmake-3.31.6:\n",
            "      Successfully uninstalled cmake-3.31.6\n",
            "Successfully installed cmake-4.0.2 ninja-1.11.1.4\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.2.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath) (4.13.2)\n",
            "Collecting portalocker (from iopath)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=15d43e8c9c047c2ecda15d1fda2b880ace9d136bb882dde2096d7b0fadd36fce\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=bd98c1755c37fc0c7952bbf993005262e84168cc624589cd315b14049102204b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -U cmake ninja\n",
        "!pip install fvcore iopath\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WNViPp8JzCT5",
        "outputId": "6345e29d-eabc-4154-f780-f0eb1195ccbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/pytorch3d.git@stable\n",
            "  Cloning https://github.com/facebookresearch/pytorch3d.git (to revision stable) to /tmp/pip-req-build-yunyiwx1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-yunyiwx1\n",
            "  Running command git checkout -q 75ebeeaea0908c5527e7b1e305fbc7681382db47\n",
            "  Resolved https://github.com/facebookresearch/pytorch3d.git to commit 75ebeeaea0908c5527e7b1e305fbc7681382db47\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (from pytorch3d==0.7.8) (0.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (4.13.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (3.1.1)\n",
            "Building wheels for collected packages: pytorch3d\n",
            "  Building wheel for pytorch3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch3d: filename=pytorch3d-0.7.8-cp311-cp311-linux_x86_64.whl size=60074361 sha256=09022837783a1b2c4acb452065cb7f50e6abbea538bceb5abe091686f4e6f25b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2vdbiq1j/wheels/08/90/1b/df18c3e3634f86278e793b87f37ea4c58d0c36731196122518\n",
            "Successfully built pytorch3d\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.8\n"
          ]
        }
      ],
      "source": [
        "#Install the newest stable version of pytorch3d It takes awhile this is normal\n",
        "!pip install \"git+https://github.com/facebookresearch/pytorch3d.git@stable\"\n",
        "#It will get stuck on building wheels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SDUU4_c1xtCO",
        "outputId": "213238dd-8080-4b5f-e7b5-54cf93f205d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7.8\n"
          ]
        }
      ],
      "source": [
        "import pytorch3d\n",
        "print(pytorch3d.__version__)\n",
        "#make sure its the following version\n",
        "#0.7.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eZX2ATSaRn5-",
        "outputId": "1df12cee-00de-451f-92c6-fc3457353328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trimesh\n",
            "  Downloading trimesh-4.6.8-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from trimesh) (2.0.2)\n",
            "Downloading trimesh-4.6.8-py3-none-any.whl (709 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/709.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/709.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.3/709.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trimesh\n",
            "Successfully installed trimesh-4.6.8\n"
          ]
        }
      ],
      "source": [
        "!pip install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "VBmJqHSvu1p5",
        "outputId": "64b398d9-e801-4317-986d-4a70723acede",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0.dev20241112+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uTYwJLMKRA0k",
        "outputId": "bd3876c2-a23d-4dd1-c5d6-e8042538ac97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/3dgs\n"
          ]
        }
      ],
      "source": [
        "# Partially from: https://github.com/yanx27/Pointnet_Pointnet2_pytorch\n",
        "import argparse\n",
        "import os\n",
        "import datetime\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import importlib\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data.sampler as sampler\n",
        "\n",
        "# Add this patch to fix NumPy issue with PyTorch DataLoader\n",
        "import torch.utils.data.sampler as sampler\n",
        "# Add this patch to fix NumPy issue with PyTorch DataLoader\n",
        "import torch.utils.data.sampler as sampler\n",
        "\n",
        "%cd /content/drive/MyDrive/3dgs\n",
        "#Change to your repo\n",
        "\n",
        "# Custom imports\n",
        "from data_utils.extra_feature import ExtraFeature\n",
        "from datasets.base_3dgs_dataset import Base3DGSDataset\n",
        "from datasets.composed_mesh_dataset import ComposedMeshDataset\n",
        "from datasets.composed_3dgs_dataset import Composed3DGSDataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define paths\n"
      ],
      "metadata": {
        "id": "qs6iXDvz_R3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RngAxBqksrqB"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "BASE_DIR = \"/content/drive/MyDrive/3dgs\"  # Path for Google Colab\n",
        "experiment_dir = BASE_DIR\n",
        "ROOT_DIR = BASE_DIR\n",
        "dataset_path = \"/content/drive/MyDrive/3dgs/datasets\"  # Path to datasets directory\n",
        "data_path = \"/content/drive/MyDrive/3dgs\"  # Path to where train.txt and test.txt are located\n",
        "test_txt = data_path + \"/data/test.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing and Class Configuration"
      ],
      "metadata": {
        "id": "Rcg-NpeJ_N7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xXQXV7wnsIdo",
        "outputId": "8f5a80ac-ca3a-4aa2-b01d-6141e55fafa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted Categories from Dataset: ['bathtub', 'bed', 'chair', 'desk', 'dresser', 'monitor', 'night_stand', 'sofa', 'table', 'toilet']\n",
            "Class to label mapping: {'bathtub': 0, 'bed': 1, 'chair': 2, 'desk': 3, 'dresser': 4, 'monitor': 5, 'night_stand': 6, 'sofa': 7, 'table': 8, 'toilet': 9}\n"
          ]
        }
      ],
      "source": [
        "#new pandas implimentation\n",
        "\n",
        "#Constructing the paths (combined train + test version)\n",
        "def modify_paths(file_txt):\n",
        "    delimiter = '/'\n",
        "    df = pd.read_csv(file_txt, delimiter=delimiter, header=None, names=[\"col1\", \"col2\"])\n",
        "\n",
        "    # Construct paths to point cloud files\n",
        "    df[\"combined\"] = dataset_path + \"/\" + df[\"col1\"] + \"/\" + df[\"col2\"] + \"/point_cloud/iteration_15000/point_cloud.ply\"\n",
        "\n",
        "    return df[\"combined\"].tolist(), df[\"col1\"].tolist(), df\n",
        "\n",
        "#Read class names from DataFrame\n",
        "def get_classes_from_df(df):\n",
        "    categories_set = set(df[\"col1\"])  # Get unique class names\n",
        "    sorted_classes = sorted(categories_set)  # Sort for consistency\n",
        "\n",
        "    print(f\"Sorted Categories from Dataset: {sorted_classes}\")\n",
        "    return sorted_classes\n",
        "\n",
        "#Set up class names and mappings\n",
        "train_paths, train_labels, train_df = modify_paths(data_path + \"/data/train.txt\")\n",
        "test_paths, test_labels, test_df = modify_paths(data_path + \"/data/test.txt\")\n",
        "CLASSES = get_classes_from_df(test_df)\n",
        "class2label = {cls: i for i, cls in enumerate(CLASSES)}\n",
        "CLASS2LABEL = class2label\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "print(f\"Class to label mapping: {CLASS2LABEL}\")\n",
        "\n",
        "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
        "\n",
        "seg_label_to_cat = {i: cat for i, cat in enumerate(class2label.keys())}\n",
        "\n",
        "def inplace_relu(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('ReLU') != -1:\n",
        "        m.inplace = True\n",
        "\n",
        "#ExtraFeature class if not imported from elsewhere\n",
        "class ExtraFeature:\n",
        "    @staticmethod\n",
        "    def feature_by_name(name):\n",
        "        # Simple placeholder implementation\n",
        "        return name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iD4oDME8OOqK"
      },
      "outputs": [],
      "source": [
        "#Define all extra feature constants BEFORE get_args()\n",
        "FEATURE_ROTATION_QUAT = 'rotation_quat'\n",
        "FEATURE_ROTATION_MATRIX = 'rotation_matrix'\n",
        "FEATURE_SCALE = 'scale'\n",
        "FEATURE_COVARIANCE = 'covariance'\n",
        "FEATURE_OPACITY = 'opacity'\n",
        "FEATURE_COLOR = 'color'\n",
        "FEATURE_REST = 'rest'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gnxCP-D7CWCD",
        "outputId": "989cceff-b6b6-4b58-baf4-6be3ac2f624f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "both train and testing paths are the same lenght as the lables\n"
          ]
        }
      ],
      "source": [
        "#Sanity test making sure the labels and paths have the same lenght meaning theres a l\n",
        "train_paths, trainlabels = modify_paths_train()\n",
        "test_paths, testlabels = modify_paths_test()\n",
        "testlabels\n",
        "\n",
        "\n",
        "if len(train_paths) == len(trainlabels) and len(test_paths) == len(testlabels):\n",
        "  print(\"both train and testing paths are the same lenght as the lables\")\n",
        "else:\n",
        "    print(\"PATHS ARE NOT SAME LENGHT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Setup and Configuration"
      ],
      "metadata": {
        "id": "ykRx--GQCrUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Random Sampling\n",
        "original_iter = sampler.RandomSampler.__iter__\n",
        "def patched_iter(self):\n",
        "    n = len(self.data_source)\n",
        "    if self.generator is None:\n",
        "        generator = torch.Generator()\n",
        "        generator.manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n",
        "    else:\n",
        "        generator = self.generator\n",
        "\n",
        "    if self.replacement:\n",
        "        for _ in range(self.num_samples // n):\n",
        "            for idx in torch.randint(0, n, size=(n,), generator=generator).tolist():\n",
        "                yield idx\n",
        "        for idx in torch.randint(0, n, size=(self.num_samples % n,), generator=generator).tolist():\n",
        "            yield idx\n",
        "    else:\n",
        "\n",
        "        for idx in torch.randperm(n, generator=generator).tolist():\n",
        "            yield idx\n",
        "\n",
        "sampler.RandomSampler.__iter__ = patched_iter\n",
        "\n",
        "# Function to save point clouds with class IDs\n",
        "def save_classified_pointcloud(points, class_ids, filename):\n",
        "    \"\"\"Save point cloud with class ID for each point.\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"ply\\n\")\n",
        "        f.write(\"format ascii 1.0\\n\")\n",
        "        f.write(f\"element vertex {len(points)}\\n\")\n",
        "        f.write(\"property float x\\n\")\n",
        "        f.write(\"property float y\\n\")\n",
        "        f.write(\"property float z\\n\")\n",
        "        f.write(\"property int class_id\\n\")\n",
        "        f.write(\"end_header\\n\")\n",
        "\n",
        "        for i in range(len(points)):\n",
        "            x, y, z = points[i]\n",
        "            class_id = int(class_ids[i])\n",
        "            f.write(f\"{x} {y} {z} {class_id}\\n\")\n",
        "\n",
        "# Read class names from train.txt/test.txt first\n",
        "def get_classes():\n",
        "    delimiter = '/'\n",
        "    # Read test file and extract class names\n",
        "    categories = pd.read_csv(test_txt, delimiter=delimiter, header=None, names=[\"col1\", \"col2\"])\n",
        "    categories_set = set(categories[\"col1\"])  # Get unique class names\n",
        "    sorted_classes = sorted(categories_set)  # Sort for consistency\n",
        "\n",
        "    print(f\"Sorted Categories from Dataset: {sorted_classes}\")\n",
        "    return sorted_classes\n",
        "\n",
        "# Set up class names and mappings\n",
        "CLASSES = get_classes()\n",
        "class2label = {cls: i for i, cls in enumerate(CLASSES)}\n",
        "CLASS2LABEL = class2label\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "print(f\"Class to label mapping: {CLASS2LABEL}\")\n",
        "\n",
        "seg_classes = class2label\n",
        "seg_label_to_cat = {}\n",
        "for i, cat in enumerate(seg_classes.keys()):\n",
        "    seg_label_to_cat[i] = cat\n",
        "\n",
        "def inplace_relu(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('ReLU') != -1:\n",
        "        m.inplace = True\n",
        "\n",
        "# Define ExtraFeature class if not imported from elsewhere\n",
        "class ExtraFeature:\n",
        "    @staticmethod\n",
        "    def feature_by_name(name):\n",
        "        # Simple placeholder implementation\n",
        "        return name\n",
        "\n",
        "\n",
        "class TrainEnv:\n",
        "    def log_string(self, str):\n",
        "        self.logger.info(str)\n",
        "        print(str)\n",
        "\n",
        "def create_environment(args, train_paths, test_paths, train_labels, test_labels):\n",
        "    env = TrainEnv()\n",
        "\n",
        "    '''HYPER PARAMETER'''\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "\n",
        "    '''CREATE DIR'''\n",
        "    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
        "    experiment_dir = Path('./log/')\n",
        "    experiment_dir.mkdir(exist_ok=True)\n",
        "    experiment_dir = experiment_dir.joinpath('sem_seg')\n",
        "    experiment_dir.mkdir(exist_ok=True)\n",
        "    if args.log_dir is None:\n",
        "        experiment_dir = experiment_dir.joinpath(timestr)\n",
        "    else:\n",
        "        experiment_dir = experiment_dir.joinpath(args.log_dir)\n",
        "    experiment_dir.mkdir(exist_ok=True)\n",
        "    env.checkpoints_dir = experiment_dir.joinpath('checkpoints/')\n",
        "    env.checkpoints_dir.mkdir(exist_ok=True)\n",
        "    log_dir = experiment_dir.joinpath('logs/')\n",
        "    log_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    '''LOG'''\n",
        "    env.logger = logging.getLogger(\"Model\")\n",
        "    env.logger.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n",
        "    file_handler.setLevel(logging.INFO)\n",
        "    file_handler.setFormatter(formatter)\n",
        "    env.logger.addHandler(file_handler)\n",
        "    env.log_string('PARAMETER ...')\n",
        "    env.log_string(args)\n",
        "\n",
        "    env.writer = SummaryWriter(log_dir=str(experiment_dir.joinpath('tensorboard')))\n",
        "\n",
        "    num_point = args.npoint\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    sampling = args.sampling\n",
        "\n",
        "    print(\"start loading training data ...\")\n",
        "    if args.dataset_type == '3DGS':\n",
        "        from datasets.composed_3dgs_dataset import Composed3DGSDataset\n",
        "\n",
        "        env.train_dataset = Composed3DGSDataset(\n",
        "            model_paths=train_paths,\n",
        "            class2label=CLASS2LABEL,  # Using class2label for label mapping\n",
        "            sampling=sampling,\n",
        "            num_point=num_point,\n",
        "            extra_features=args.extra_features  # Pass the `ExtraFeature` objects here\n",
        "        )\n",
        "    elif args.dataset_type == 'SampledMesh':\n",
        "        from datasets.composed_mesh_dataset import ComposedMeshDataset\n",
        "\n",
        "        env.train_dataset = ComposedMeshDataset(model_paths=train_paths, class2label=CLASS2LABEL, num_point=num_point)\n",
        "\n",
        "    print(\"start loading test data ...\")\n",
        "    if args.dataset_type == '3DGS':\n",
        "        env.test_dataset = Composed3DGSDataset(\n",
        "            model_paths=test_paths,\n",
        "            class2label=CLASS2LABEL,  # Using class2label for label mapping\n",
        "            sampling=sampling,\n",
        "            num_point=num_point,\n",
        "            extra_features=args.extra_features  # Pass the `ExtraFeature` objects here\n",
        "        )\n",
        "    elif args.dataset_type == 'SampledMesh':\n",
        "        env.test_dataset = ComposedMeshDataset(model_paths=test_paths, class2label=CLASS2LABEL, num_point=num_point)\n",
        "\n",
        "    def custom_worker_init_fn(worker_id):\n",
        "        # Use PyTorch's random number generator instead of NumPy's\n",
        "        worker_seed = torch.initial_seed() % 2**32\n",
        "        torch.manual_seed(worker_seed)\n",
        "        # Only set NumPy seed if it's safe to do so\n",
        "        try:\n",
        "            np.random.seed(worker_seed)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    env.trainDataLoader = torch.utils.data.DataLoader(\n",
        "        env.train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=False,  # Disabled pin_memory\n",
        "        drop_last=True,\n",
        "        num_workers=4,\n",
        "        worker_init_fn=custom_worker_init_fn\n",
        "    )\n",
        "\n",
        "    env.testDataLoader = torch.utils.data.DataLoader(\n",
        "        env.test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        pin_memory=False,  # Disabled pin_memory\n",
        "        drop_last=True,\n",
        "        num_workers=4,\n",
        "        worker_init_fn=custom_worker_init_fn\n",
        "    )\n",
        "\n",
        "    env.weights = torch.Tensor(env.train_dataset.label_weights).cuda()\n",
        "\n",
        "    env.log_string(\"The number of training data is: %d\" % len(env.train_dataset))\n",
        "    env.log_string(\"The number of test data is: %d\" % len(env.test_dataset))\n",
        "\n",
        "    '''MODEL LOADING'''\n",
        "    MODEL = importlib.import_module(args.model)\n",
        "\n",
        "    env.classifier = MODEL.get_model(NUM_CLASSES, env.train_dataset.get_channels_count).cuda()\n",
        "    env.classifier.apply(inplace_relu)\n",
        "    env.criterion = MODEL.get_loss().cuda()\n",
        "\n",
        "    def weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv2d') != -1:\n",
        "            torch.nn.init.xavier_normal_(m.weight.data)\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('Linear') != -1:\n",
        "            torch.nn.init.xavier_normal_(m.weight.data)\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(str(experiment_dir) + '/checkpoints/model.pth')\n",
        "        env.start_epoch = checkpoint['epoch'] + 1\n",
        "        env.classifier.load_state_dict(checkpoint['model_state_dict'])\n",
        "        env.log_string('Use pretrain model')\n",
        "    except:\n",
        "        env.log_string('No existing model, starting from scratch...')\n",
        "        env.start_epoch = 0\n",
        "        env.classifier = env.classifier.apply(weights_init)\n",
        "\n",
        "    if args.optimizer == 'Adam':\n",
        "        env.optimizer = torch.optim.Adam(\n",
        "            env.classifier.parameters(),\n",
        "            lr=args.learning_rate,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-08,\n",
        "            weight_decay=args.weight_decay_rate\n",
        "        )\n",
        "    else:\n",
        "        env.optimizer = torch.optim.SGD(env.classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n",
        "\n",
        "    return env\n",
        "\n",
        "\n",
        "def close_environment(env):\n",
        "    env.writer.close()\n",
        "\n",
        "    handlers = env.logger.handlers[:]\n",
        "    for handler in handlers:\n",
        "        env.logger.removeHandler(handler)\n",
        "        handler.close()\n",
        "        #Show classes"
      ],
      "metadata": {
        "id": "EBbrnu4RCC1n",
        "outputId": "449f0e2f-5740-412d-9e7c-d3264aff2fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted Categories from Dataset: ['bathtub', 'bed', 'chair', 'desk', 'dresser', 'monitor', 'night_stand', 'sofa', 'table', 'toilet']\n",
            "Class to label mapping: {'bathtub': 0, 'bed': 1, 'chair': 2, 'desk': 3, 'dresser': 4, 'monitor': 5, 'night_stand': 6, 'sofa': 7, 'table': 8, 'toilet': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PointNet segmentation code I modified, with help from Claude AI\n",
        "I took the original PointNet segmentation code and made several improvements:\n",
        "- Streamlined how variables are declared for better organization\n",
        "- Added functionality to save point cloud visualization as PLY files\n",
        "- Created the EvalResults class for cleaner metric tracking\n",
        "\n",
        "Claude helped me troubleshoot the PLY file saving implementation and\n",
        "suggested refinements to the visualization approach. The core structure\n",
        "is from PointNet, but my modifications make it more usable for my needs."
      ],
      "metadata": {
        "id": "LUOkdBJX-uHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Declare your aguments here"
      ],
      "metadata": {
        "id": "uWerDt93EBKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change your arguments here\n",
        "def get_args():\n",
        "    args = argparse.Namespace()\n",
        "\n",
        "    args.data_path = \"/content/drive/MyDrive/3dgs/datasets\" #Change to your path\n",
        "    args.model = 'pointnet2_sem_seg'\n",
        "    args.dataset_type = '3DGS'\n",
        "    args.batch_size = 8\n",
        "    args.epoch = 200\n",
        "    args.learning_rate = 0.003\n",
        "    args.gpu = '0'\n",
        "    args.optimizer = 'Adam'\n",
        "    args.log_dir = 'epochs250_learningrate_003_bs32_v027_001_4096' #rename each time overwise the model will treat the previous version as a checkpoint\n",
        "    args.weight_decay_rate = 1e-4\n",
        "    args.npoint = 4096\n",
        "    args.lr_step_size = 5  # how often it updates\n",
        "    args.lr_decay = 0.95\n",
        "    args.eval_after_epoch = True\n",
        "    args.sampling = 'uniform'\n",
        "\n",
        "    # Test mode settings this is just to see if it will train and test (not good results)\n",
        "    args.test_mode = True #I added this so you can test a smaller sample size to see if the labeling runs and goes onto the next section\n",
        "    args.max_test_samples = 200\n",
        "    # Predefined extra features\n",
        "    predefined_extra_features = [\n",
        "        'rotation_quat',\n",
        "        'scale',\n",
        "        'opacity',\n",
        "    ]\n",
        "\n",
        "    # Convert to ExtraFeature objects\n",
        "    from data_utils.extra_feature import ExtraFeature\n",
        "    args.extra_features = [ExtraFeature.feature_by_name(feature) for feature in predefined_extra_features] if predefined_extra_features else None\n",
        "    return args\n"
      ],
      "metadata": {
        "id": "59Zi6VSHD9FI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation\n"
      ],
      "metadata": {
        "id": "80U2-d_FCmAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, args):\n",
        "    def bn_momentum_adjust(m, momentum):\n",
        "        if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n",
        "            m.momentum = momentum\n",
        "\n",
        "    # Set some constants for the learning scheduler\n",
        "    LEARNING_RATE_CLIP = 1e-5\n",
        "    MOMENTUM_ORIGINAL = 0.1\n",
        "    MOMENTUM_DECCAY = 0.5\n",
        "    MOMENTUM_DECCAY_STEP = args.lr_step_size\n",
        "\n",
        "    global_epoch = 0\n",
        "    best_iou = 0\n",
        "\n",
        "    #Main training loop\n",
        "    for epoch in range(env.start_epoch, args.epoch):\n",
        "        env.log_string('**** Epoch %d (%d/%s) ****' % (global_epoch + 1, epoch + 1, args.epoch))\n",
        "\n",
        "        # Decay learning rate over time\n",
        "        lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.lr_step_size)), LEARNING_RATE_CLIP)\n",
        "        env.log_string('Learning rate:%f' % lr)\n",
        "        for param_group in env.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        #Update batch norm momentum - helps stabilize training\n",
        "        momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n",
        "        if momentum < 0.01:\n",
        "            momentum = 0.01\n",
        "        print('BN momentum updated to: %f' % momentum)\n",
        "        env.classifier = env.classifier.apply(lambda x: bn_momentum_adjust(x, momentum))\n",
        "\n",
        "        env.writer.add_scalar('LR', lr, epoch)\n",
        "\n",
        "        # Reset metrics for this epoch\n",
        "        num_batches = len(env.trainDataLoader)\n",
        "        total_correct = 0\n",
        "        total_seen = 0\n",
        "        loss_sum = 0\n",
        "        env.classifier = env.classifier.train()\n",
        "\n",
        "        # Process each batch\n",
        "        for i, (points, target) in tqdm(enumerate(env.trainDataLoader), total=len(env.trainDataLoader), smoothing=0.9):\n",
        "            env.optimizer.zero_grad()\n",
        "\n",
        "            # Prep the point cloud data\n",
        "            points = points.data.numpy()\n",
        "            points = torch.Tensor(points)\n",
        "            points, target = points.float().cuda(), target.long().cuda()\n",
        "            points = points.transpose(2, 1)  # PointNet needs this format\n",
        "\n",
        "            # Forward pass through model\n",
        "            seg_pred, trans_feat = env.classifier(points)\n",
        "            seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
        "\n",
        "            batch_label = target.view(-1, 1)[:, 0].cpu().data.numpy()\n",
        "            target = target.view(-1, 1)[:, 0]\n",
        "\n",
        "            # Calculate loss and update weights\n",
        "            loss = env.criterion(seg_pred, target, trans_feat, env.weights)\n",
        "            loss.backward()\n",
        "            env.optimizer.step()\n",
        "\n",
        "            # Track how we're doing\n",
        "            pred_choice = seg_pred.cpu().data.max(1)[1].numpy()\n",
        "            correct = np.sum(pred_choice == batch_label)\n",
        "            total_correct += correct\n",
        "            total_seen += (args.batch_size * args.npoint)\n",
        "            loss_sum += loss\n",
        "\n",
        "        # Log training results\n",
        "        training_loss = loss_sum / num_batches\n",
        "        training_accuracy = total_correct / float(total_seen)\n",
        "\n",
        "        env.log_string('Training mean loss: %f' % training_loss)\n",
        "        env.log_string('Training accuracy: %f' % training_accuracy)\n",
        "        env.writer.add_scalar('Train loss', training_loss, epoch)\n",
        "        env.writer.add_scalar('Train accuracy', training_accuracy, epoch)\n",
        "\n",
        "        # Save model checkpoints regularly\n",
        "        if epoch % 5 == 0 or epoch == args.epoch - 1:\n",
        "            env.log_string('Save model...')\n",
        "            savepath = str(env.checkpoints_dir) + '/model.pth'\n",
        "            env.log_string('Saving at %s' % savepath)\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': env.classifier.state_dict(),\n",
        "                'optimizer_state_dict': env.optimizer.state_dict(),\n",
        "            }\n",
        "            torch.save(state, savepath)\n",
        "            env.log_string('Saving model....')\n",
        "\n",
        "        # Evaluation phase\n",
        "        if args.eval_after_epoch:\n",
        "            env.log_string('---- EPOCH %03d EVALUATION ----' % (global_epoch + 1))\n",
        "            eval_results = evaluate(env, args, epoch=epoch)\n",
        "\n",
        "            #Log Progress\n",
        "            env.log_string('eval point avg class IoU: %f' % eval_results.mIoU)\n",
        "            env.log_string('eval point avg class acc: %f' % (\n",
        "                eval_results.mean_class_accuracy))\n",
        "\n",
        "            # Show per-class results\n",
        "            iou_per_class_str = '------- IoU --------\\n'\n",
        "            for l in range(NUM_CLASSES):\n",
        "                iou_per_class_str += 'class %s weight: %.3f, IoU: %.3f \\n' % (\n",
        "                    seg_label_to_cat[l] + ' ' * (14 - len(seg_label_to_cat[l])),\n",
        "                    eval_results.labelweights[l - 1],\n",
        "                    eval_results.class_mIoU[l]\n",
        "                )\n",
        "\n",
        "            env.log_string(iou_per_class_str)\n",
        "            env.log_string('Eval mean loss: %f' % eval_results.loss)\n",
        "            env.log_string('Eval accuracy: %f' % eval_results.accuracy)\n",
        "\n",
        "            env.writer.add_scalar('Eval loss', eval_results.loss, epoch)\n",
        "            env.writer.add_scalar('Eval accuracy', eval_results.accuracy, epoch)\n",
        "            env.writer.add_scalar('Eval mIoU', eval_results.mIoU, epoch)\n",
        "\n",
        "            if eval_results.mIoU >= best_iou:\n",
        "                best_iou = eval_results.mIoU\n",
        "                env.log_string('Save model...')\n",
        "                savepath = str(env.checkpoints_dir) + '/best_model.pth'\n",
        "                env.log_string('Saving at %s' % savepath)\n",
        "                state = {\n",
        "                    'epoch': epoch,\n",
        "                    'class_avg_iou': eval_results.mIoU,\n",
        "                    'model_state_dict': env.classifier.state_dict(),\n",
        "                    'optimizer_state_dict': env.optimizer.state_dict(),\n",
        "                }\n",
        "                torch.save(state, savepath)\n",
        "                env.log_string('Saving model....')\n",
        "            env.log_string('Best mIoU: %f' % best_iou)\n",
        "\n",
        "        env.writer.flush()\n",
        "        global_epoch += 1\n",
        "\n",
        "\n",
        "# Container for all our evaluation metrics\n",
        "class EvalResults:\n",
        "    def __init__(self, mIoU, loss, accuracy, labelweights, mean_class_accuracy, class_mIoU):\n",
        "        self.mIoU = mIoU\n",
        "        self.loss = loss\n",
        "        self.accuracy = accuracy\n",
        "        self.labelweights = labelweights\n",
        "        self.mean_class_accuracy = mean_class_accuracy\n",
        "        self.class_mIoU = class_mIoU\n",
        "\n",
        "\n",
        "def evaluate(env, args, epoch=None):\n",
        "    with torch.no_grad():\n",
        "        # Setup tracking variables\n",
        "        num_batches = len(env.testDataLoader)\n",
        "        total_correct = 0\n",
        "        total_seen = 0\n",
        "        loss_sum = 0\n",
        "        labelweights = np.zeros(NUM_CLASSES)\n",
        "        total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
        "        total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
        "        total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n",
        "        env.classifier = env.classifier.eval()  # Switch to eval mode\n",
        "\n",
        "        # Save visualization files every 50 epochs\n",
        "        save_visualization = (epoch is not None) and (epoch % 50 == 0 or epoch == args.epoch - 1)\n",
        "\n",
        "        # Need these lists for visualization\n",
        "        if save_visualization:\n",
        "            all_points = []\n",
        "            all_predictions = []\n",
        "            all_ground_truth = []\n",
        "\n",
        "        # Process test batches\n",
        "        for i, (points, target) in tqdm(enumerate(env.testDataLoader), total=len(env.testDataLoader), smoothing=0.9):\n",
        "            original_points = points.data.numpy()\n",
        "\n",
        "            points = torch.Tensor(original_points)\n",
        "            points, target = points.float().cuda(), target.long().cuda()\n",
        "            points = points.transpose(2, 1)\n",
        "\n",
        "            # Get model predictions\n",
        "            seg_pred, trans_feat = env.classifier(points)\n",
        "            pred_val = seg_pred.contiguous().cpu().data.numpy()\n",
        "            seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
        "\n",
        "            batch_label = target.cpu().data.numpy()\n",
        "            target = target.view(-1, 1)[:, 0]\n",
        "            loss = env.criterion(seg_pred, target, trans_feat, env.weights)\n",
        "            loss_sum += loss\n",
        "            pred_val = np.argmax(pred_val, 2)  # Get class predictions\n",
        "            correct = np.sum((pred_val == batch_label))\n",
        "            total_correct += correct\n",
        "            total_seen += (args.batch_size * args.npoint)\n",
        "            tmp, _ = np.histogram(batch_label, range(NUM_CLASSES + 1))\n",
        "            labelweights += tmp\n",
        "\n",
        "            # Per-class metrics\n",
        "            for l in range(NUM_CLASSES):\n",
        "                total_seen_class[l] += np.sum((batch_label == l))\n",
        "                total_correct_class[l] += np.sum((pred_val == l) & (batch_label == l))\n",
        "                total_iou_deno_class[l] += np.sum(((pred_val == l) | (batch_label == l)))\n",
        "\n",
        "            # Collect visualization data if needed\n",
        "            if save_visualization:\n",
        "                for b in range(original_points.shape[0]):\n",
        "                    # Just grab XYZ coordinates\n",
        "                    batch_points = original_points[b, :, :3]\n",
        "                    batch_pred = pred_val[b]\n",
        "                    batch_gt = batch_label[b]\n",
        "\n",
        "                    all_points.append(batch_points)\n",
        "                    all_predictions.append(batch_pred)\n",
        "                    all_ground_truth.append(batch_gt)\n",
        "\n",
        "        # Save colorized point clouds for visualization\n",
        "        if save_visualization:\n",
        "            vis_dir = os.path.join(str(env.checkpoints_dir), 'visualizations')\n",
        "            os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "            # Combine all the collected data\n",
        "            full_points = np.vstack(all_points)\n",
        "            full_predictions = np.concatenate(all_predictions)\n",
        "            full_ground_truth = np.concatenate(all_ground_truth)\n",
        "\n",
        "            # Save as PLY files that can be viewed in 3D software\n",
        "            pred_filename = os.path.join(vis_dir, f'epoch_{epoch}_full_prediction.ply')\n",
        "            save_classified_pointcloud(full_points, full_predictions, pred_filename)\n",
        "\n",
        "            gt_filename = os.path.join(vis_dir, f'epoch_{epoch}_full_groundtruth.ply')\n",
        "            save_classified_pointcloud(full_points, full_ground_truth, gt_filename)\n",
        "\n",
        "        # Calculate final metrics\n",
        "        labelweights = labelweights.astype(np.float32) / np.sum(labelweights.astype(np.float32))\n",
        "        mIoU = np.mean(np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=float) + 1e-6))\n",
        "        eval_loss = loss_sum / float(num_batches)\n",
        "        eval_accuracy = total_correct / float(total_seen)\n",
        "        mean_class_accuracy = np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=float) + 1e-6))\n",
        "        class_mIoU = [total_correct_class[l] / float(total_iou_deno_class[l]) for l in range(NUM_CLASSES)]\n",
        "\n",
        "        return EvalResults(mIoU, eval_loss, eval_accuracy, labelweights, mean_class_accuracy, class_mIoU)\n",
        "\n",
        "def main():\n",
        "    # Get the config settings\n",
        "    args = get_args()\n",
        "\n",
        "    # Load dataset paths and their labels\n",
        "    train_paths, trainlabels = modify_paths_train()\n",
        "    test_paths, testlabels = modify_paths_test()\n",
        "\n",
        "    # Use fewer samples in test mode\n",
        "    if args.test_mode:\n",
        "        print(f\"Test mode enabled - using only {args.max_test_samples} samples\")\n",
        "        train_paths = train_paths[:args.max_test_samples]\n",
        "        trainlabels = trainlabels[:args.max_test_samples]\n",
        "        test_paths = test_paths[:args.max_test_samples]\n",
        "        testlabels = testlabels[:args.max_test_samples]\n",
        "\n",
        "    # Make sure our data is valid\n",
        "    if len(train_paths) == len(trainlabels) and len(test_paths) == len(testlabels):\n",
        "        # Setup and run training\n",
        "        env = create_environment(args, train_paths, test_paths, trainlabels, testlabels)\n",
        "        train(env, args)\n",
        "        close_environment(env)\n",
        "    else:\n",
        "        # Something's wrong with the data\n",
        "        raise Exception(\"The list of labels for the train and test data do not match lengths.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "FZ8pcQlfCi1I",
        "outputId": "6cd09c75-e122-461e-feeb-970497cd8011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Model:PARAMETER ...\n",
            "INFO:Model:Namespace(data_path='/content/drive/MyDrive/3dgs/datasets', model='pointnet2_sem_seg', dataset_type='3DGS', batch_size=8, epoch=200, learning_rate=0.003, gpu='0', optimizer='Adam', log_dir='epochs250_learningrate_003_bs32_v027_001_4096', weight_decay_rate=0.0001, npoint=4096, lr_step_size=5, lr_decay=0.95, eval_after_epoch=True, sampling='uniform', test_mode=True, max_test_samples=200, extra_features=[<data_utils.extra_feature.ExtraFeature object at 0x78fde5c057d0>, <data_utils.extra_feature.ExtraFeature object at 0x78fdc9b65010>, <data_utils.extra_feature.ExtraFeature object at 0x78fdc9b67350>])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test mode enabled - using only 200 samples\n",
            "PARAMETER ...\n",
            "Namespace(data_path='/content/drive/MyDrive/3dgs/datasets', model='pointnet2_sem_seg', dataset_type='3DGS', batch_size=8, epoch=200, learning_rate=0.003, gpu='0', optimizer='Adam', log_dir='epochs250_learningrate_003_bs32_v027_001_4096', weight_decay_rate=0.0001, npoint=4096, lr_step_size=5, lr_decay=0.95, eval_after_epoch=True, sampling='uniform', test_mode=True, max_test_samples=200, extra_features=[<data_utils.extra_feature.ExtraFeature object at 0x78fde5c057d0>, <data_utils.extra_feature.ExtraFeature object at 0x78fdc9b65010>, <data_utils.extra_feature.ExtraFeature object at 0x78fdc9b67350>])\n",
            "start loading training data ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 3/200 [00:00<00:07, 27.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0001/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0002/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0003/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0004/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0005/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0006/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0007/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 12/200 [00:00<00:05, 34.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0008/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0009/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0010/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0011/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0012/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0013/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0014/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0015/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0016/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 21/200 [00:00<00:04, 35.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0017/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0018/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0019/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0020/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0021/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0022/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0023/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▎        | 25/200 [00:00<00:05, 32.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0024/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0025/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0026/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0027/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0028/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0029/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▋        | 33/200 [00:00<00:05, 32.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0030/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0031/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0032/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0033/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0034/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0035/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0036/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0037/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0038/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 42/200 [00:01<00:04, 36.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0039/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0040/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0041/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0042/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0043/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0044/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0045/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 50/200 [00:01<00:04, 32.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0046/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0047/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0048/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0049/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0050/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0051/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0052/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 55/200 [00:01<00:04, 33.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0053/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0054/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0055/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0056/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0057/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0058/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0059/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 63/200 [00:01<00:03, 34.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0060/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0061/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0062/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0063/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0064/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0065/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0066/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 72/200 [00:02<00:03, 36.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0067/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0068/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0069/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0070/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0071/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0072/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0073/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0074/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0075/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 76/200 [00:02<00:03, 35.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0076/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0077/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0078/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0079/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0080/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 80/200 [00:02<00:03, 35.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0081/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0082/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0083/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 84/200 [00:02<00:03, 34.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0084/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0085/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0086/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0087/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 88/200 [00:02<00:03, 34.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0088/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0089/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0090/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 92/200 [00:02<00:03, 32.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0091/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0092/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0093/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 96/200 [00:02<00:03, 33.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0094/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0095/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0096/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0097/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 100/200 [00:02<00:02, 33.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0098/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0099/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0100/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0101/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 104/200 [00:03<00:02, 34.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0102/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0103/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0104/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0105/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0106/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0001/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 108/200 [00:03<00:03, 28.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0002/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0003/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0004/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0005/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0006/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 112/200 [00:03<00:03, 27.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0007/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0008/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0009/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▊    | 115/200 [00:03<00:03, 24.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0010/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 118/200 [00:03<00:03, 21.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0011/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0012/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0013/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 121/200 [00:03<00:03, 22.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0014/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0015/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0016/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 124/200 [00:03<00:03, 23.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0017/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0018/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0019/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0020/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0021/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▎   | 127/200 [00:04<00:03, 22.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0022/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0023/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 130/200 [00:04<00:03, 22.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0024/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0025/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0026/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▋   | 133/200 [00:04<00:02, 24.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0027/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0028/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 136/200 [00:04<00:02, 22.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0029/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0030/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0031/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0032/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0033/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 142/200 [00:04<00:02, 23.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0034/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0035/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0036/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0037/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0038/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0039/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▎  | 145/200 [00:04<00:02, 23.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0040/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0041/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0042/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 148/200 [00:05<00:02, 22.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0043/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 151/200 [00:05<00:02, 21.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0044/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0045/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0046/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0047/point_cloud/iteration_15000/point_cloud.ply\n",
            "classname: bed, path: /content/drive/MyDrive/3dgs/datasets/bed/bed_0048/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▋  | 153/200 [00:05<00:01, 28.88it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-18f36b1ced4d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-18f36b1ced4d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_paths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_paths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Setup and run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mclose_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-6cc3378aa5b3>\u001b[0m in \u001b[0;36mcreate_environment\u001b[0;34m(args, train_paths, test_paths, train_labels, test_labels)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomposed_3dgs_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComposed3DGSDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         env.train_dataset = Composed3DGSDataset(\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mmodel_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mclass2label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASS2LABEL\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Using class2label for label mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/3dgs/datasets/composed_3dgs_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_paths, class2label, sampling, num_point, extra_features)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mComposed3DGSDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBase3DGSDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_into_random_subsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_subset_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_subset_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/3dgs/datasets/base_3dgs_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_paths, class2label, sampling, num_point, extra_features)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_features\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/3dgs/datasets/base_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_paths, class2label, num_point)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/3dgs/datasets/base_3dgs_dataset.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_path, label)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mextra_feature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         model = (GaussianModel.load_from(model_path, extra_feature_names)\n\u001b[0m\u001b[1;32m     15\u001b[0m                  \u001b[0;34m.\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                  .with_label(label))\n",
            "\u001b[0;32m/content/drive/MyDrive/3dgs/data_utils/gaussian_model.py\u001b[0m in \u001b[0;36mload_from\u001b[0;34m(cls, path, extra_features)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mextra_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_ply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/3dgs/data_utils/gaussian_model.py\u001b[0m in \u001b[0;36mload_ply\u001b[0;34m(self, path, extra_features)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mplydata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlyData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         self._xyz = np.stack((\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplydata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplydata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m     return _nx.concatenate(expanded_arrays, axis=axis, out=out,\n\u001b[0m\u001b[1;32m    456\u001b[0m                            dtype=dtype, casting=casting)\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1_nlGgzlxcE5SWaBOGG1inlpsHKJgancb",
      "authorship_tag": "ABX9TyMfDXPPR2PXtn6vSNFs3A6j",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}