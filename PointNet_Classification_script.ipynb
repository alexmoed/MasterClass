{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexmoed/MasterClass/blob/3D-Pointnet%2B%2B/PointNet_Classification_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pnOKJBB5xk79"
      },
      "outputs": [],
      "source": [
        " # !git clone https://github.com/karol-202/direct-3dgs-segmentation \"/content/drive/MyDrive/3dgs/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8Ax15U0J8pB",
        "outputId": "91e5d5da-60e5-4133-a28f-348a600195cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#change the directory to the correct spot on google drive\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train a semantic segmentation model for 3D Gaussian Splats\n",
        "# Modified from :\n",
        "# Yanx27 (2019). PointNet_Pointnet2_pytorch [online].\n",
        "# [Accessed 2024]. Available from: \"https://github.com/yanx27/Pointnet_Pointnet2_pytorch\"\n",
        "# Based on research from:\n",
        "# Jurski, K. (2024). Semantic 3D segmentation of 3D Gaussian Splats: Assessing existing\n",
        "# point cloud segmentation techniques on semantic segmentation of synthetic 3D Gaussian\n",
        "# Splats scenes. Bachelor's Thesis, Delft University of Technology.\n",
        "# Extended implementation: \"https://github.com/karol-202/direct-3dgs-segmentation\""
      ],
      "metadata": {
        "id": "wNHhvtbP9qwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PointNet ++ for Gaussian splats"
      ],
      "metadata": {
        "id": "w7gebjNX7XYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing depencencies"
      ],
      "metadata": {
        "id": "EJCG4e5x7cQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciTW75Hh_6_w",
        "outputId": "32ba08be-8d9e-4a8d-96da-53d7c0ae714e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plyfile in /usr/local/lib/python3.11/dist-packages (1.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from plyfile) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install plyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlodEOZnyQWS",
        "outputId": "6e22bb90-a01b-4a02-e3ff-eba14cef9159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0.dev20241112+cu121\n",
            "Uninstalling torch-2.6.0.dev20241112+cu121:\n",
            "  Successfully uninstalled torch-2.6.0.dev20241112+cu121\n",
            "Found existing installation: torchvision 0.20.0.dev20241112+cu121\n",
            "Uninstalling torchvision-0.20.0.dev20241112+cu121:\n",
            "  Successfully uninstalled torchvision-0.20.0.dev20241112+cu121\n",
            "Found existing installation: torchaudio 2.5.0.dev20241112+cu121\n",
            "Uninstalling torchaudio-2.5.0.dev20241112+cu121:\n",
            "  Successfully uninstalled torchaudio-2.5.0.dev20241112+cu121\n",
            "Found existing installation: pytorch3d 0.7.8\n",
            "Uninstalling pytorch3d-0.7.8:\n",
            "  Successfully uninstalled pytorch3d-0.7.8\n"
          ]
        }
      ],
      "source": [
        "#Uninstall any versions of pytorch3d\n",
        "!pip uninstall -y torch torchvision torchaudio pytorch3d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "egDDaEHnyT9X",
        "outputId": "f8619eff-9f7f-4f0c-934d-7b878dbf2776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu121/torch-2.6.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (768.0 MB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu121/torchvision-0.20.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (7.4 MB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu121/torchaudio-2.5.0.dev20241112%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: pytorch-triton==3.1.0+cf34004b8a in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0+cf34004b8a)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-2.6.0.dev20241112+cu121 torchaudio-2.5.0.dev20241112+cu121 torchvision-0.20.0.dev20241112+cu121\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "04192831694b40cea76ce7c169e1f9cb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Run this exact version of torch vision others dont seem to work with pytorch 3d\n",
        "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dSilur4IzHSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588ef573-6e0d-4375-cab1-6401ddf4d348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (1.11.1.4)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.11/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (0.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.0.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.2.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath) (4.13.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U cmake ninja\n",
        "!pip install fvcore iopath\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WNViPp8JzCT5",
        "outputId": "baf83327-364e-4c63-8037-219b1ae5a91e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/pytorch3d.git@stable\n",
            "  Cloning https://github.com/facebookresearch/pytorch3d.git (to revision stable) to /tmp/pip-req-build-snfrurta\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-snfrurta\n",
            "  Running command git checkout -q 75ebeeaea0908c5527e7b1e305fbc7681382db47\n",
            "  Resolved https://github.com/facebookresearch/pytorch3d.git to commit 75ebeeaea0908c5527e7b1e305fbc7681382db47\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (from pytorch3d==0.7.8) (0.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (4.13.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (3.1.1)\n",
            "Building wheels for collected packages: pytorch3d\n",
            "  Building wheel for pytorch3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch3d: filename=pytorch3d-0.7.8-cp311-cp311-linux_x86_64.whl size=60074661 sha256=d9d07321b6363a11a2148cb24e240cec47336365080576f7c0da3dfdafbe1c49\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9ikemka0/wheels/08/90/1b/df18c3e3634f86278e793b87f37ea4c58d0c36731196122518\n",
            "Successfully built pytorch3d\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.8\n"
          ]
        }
      ],
      "source": [
        "#Install the newest stable version of pytorch3d It takes awhile this is normal\n",
        "!pip install \"git+https://github.com/facebookresearch/pytorch3d.git@stable\"\n",
        "#It will get stuck on building wheels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SDUU4_c1xtCO",
        "outputId": "afc2cc43-d1b5-4e9b-92c0-a0e1826a07af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7.8\n"
          ]
        }
      ],
      "source": [
        "import pytorch3d\n",
        "print(pytorch3d.__version__)\n",
        "#make sure its the following version\n",
        "#0.7.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eZX2ATSaRn5-",
        "outputId": "6b251d82-6f2e-4150-b8e2-3144015e64da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.11/dist-packages (4.6.8)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from trimesh) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "VBmJqHSvu1p5",
        "outputId": "1dbdae67-90e6-477f-a76a-6b0f3b239cae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0.dev20241112+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uTYwJLMKRA0k",
        "outputId": "b335d04c-02d3-4f23-cc74-68be01aa7081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/3dgs\n"
          ]
        }
      ],
      "source": [
        "# Partially from: https://github.com/yanx27/Pointnet_Pointnet2_pytorch\n",
        "import argparse\n",
        "import os\n",
        "import datetime\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import importlib\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data.sampler as sampler\n",
        "\n",
        "# Add this patch to fix NumPy issue with PyTorch DataLoader\n",
        "import torch.utils.data.sampler as sampler\n",
        "# Add this patch to fix NumPy issue with PyTorch DataLoader\n",
        "import torch.utils.data.sampler as sampler\n",
        "\n",
        "%cd /content/drive/MyDrive/3dgs\n",
        "#Change to your repo\n",
        "\n",
        "# Custom imports\n",
        "from data_utils.extra_feature import ExtraFeature\n",
        "from datasets.base_3dgs_dataset import Base3DGSDataset\n",
        "from datasets.composed_mesh_dataset import ComposedMeshDataset\n",
        "from datasets.composed_3dgs_dataset import Composed3DGSDataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define paths\n"
      ],
      "metadata": {
        "id": "qs6iXDvz_R3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RngAxBqksrqB"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "BASE_DIR = \"/content/drive/MyDrive/3dgs\"  # Path for Google Colab\n",
        "experiment_dir = BASE_DIR\n",
        "ROOT_DIR = BASE_DIR\n",
        "dataset_path = \"/content/drive/MyDrive/3dgs/datasets\"  # Path to datasets directory\n",
        "data_path = \"/content/drive/MyDrive/3dgs\"  # Path to where train.txt and test.txt are located\n",
        "test_txt = data_path + \"/data/test.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing and Class Configuration"
      ],
      "metadata": {
        "id": "Rcg-NpeJ_N7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xXQXV7wnsIdo",
        "outputId": "74925f4c-c659-40a9-8d59-6feafd08ce96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted Categories from Dataset: ['bathtub', 'bed', 'chair', 'desk', 'dresser', 'monitor', 'night_stand', 'sofa', 'table', 'toilet']\n",
            "Class to label mapping: {'bathtub': 0, 'bed': 1, 'chair': 2, 'desk': 3, 'dresser': 4, 'monitor': 5, 'night_stand': 6, 'sofa': 7, 'table': 8, 'toilet': 9}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Function to save point clouds with class IDs\n",
        "def save_classified_pointcloud(points, class_ids, filename):\n",
        "    \"\"\"Save point cloud with class ID for each point.\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"ply\\n\")\n",
        "        f.write(\"format ascii 1.0\\n\")\n",
        "        f.write(f\"element vertex {len(points)}\\n\")\n",
        "        f.write(\"property float x\\n\")\n",
        "        f.write(\"property float y\\n\")\n",
        "        f.write(\"property float z\\n\")\n",
        "        f.write(\"property int class_id\\n\")\n",
        "        f.write(\"end_header\\n\")\n",
        "\n",
        "        for i in range(len(points)):\n",
        "            x, y, z = points[i]\n",
        "            class_id = int(class_ids[i])\n",
        "            f.write(f\"{x} {y} {z} {class_id}\\n\")\n",
        "\n",
        "#Read class names from train.txt/test.txt first\n",
        "def get_classes():\n",
        "    delimiter = '/'\n",
        "    test_txt\n",
        "\n",
        "    #Read test file and extract class names\n",
        "    categories = pd.read_csv(test_txt, delimiter=delimiter, header=None, names=[\"col1\", \"col2\"])\n",
        "    categories_set = set(categories[\"col1\"])  # Get unique class names\n",
        "    sorted_classes = sorted(categories_set)  # Sort for consistency\n",
        "\n",
        "    print(f\"Sorted Categories from Dataset: {sorted_classes}\")\n",
        "    return sorted_classes\n",
        "\n",
        "#Set up class names and mappings\n",
        "CLASSES = get_classes()\n",
        "class2label = {cls: i for i, cls in enumerate(CLASSES)}\n",
        "CLASS2LABEL = class2label\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "print(f\"Class to label mapping: {CLASS2LABEL}\")\n",
        "\n",
        "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
        "\n",
        "seg_classes = class2label\n",
        "seg_label_to_cat = {}\n",
        "for i, cat in enumerate(seg_classes.keys()):\n",
        "    seg_label_to_cat[i] = cat\n",
        "\n",
        "\n",
        "def inplace_relu(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('ReLU') != -1:\n",
        "        m.inplace = True\n",
        "\n",
        "\n",
        "#Define ExtraFeature class if not imported from elsewhere\n",
        "class ExtraFeature:\n",
        "    @staticmethod\n",
        "    def feature_by_name(name):\n",
        "        # Simple placeholder implementation\n",
        "        return name\n",
        "\n",
        "\n",
        "#Constructing the paths\n",
        "def modify_paths_train():\n",
        "    delimiter = '/'\n",
        "    train_txt = data_path + \"/data/train.txt\"\n",
        "\n",
        "    # Read train file and split into two columns\n",
        "    train_df = pd.read_csv(train_txt, delimiter=delimiter, header=None, names=[\"col1\", \"col2\"])\n",
        "\n",
        "    # Construct paths to point cloud files\n",
        "    train_df[\"combined\"] = dataset_path + \"/\" + train_df[\"col1\"] + \"/\" + train_df[\"col2\"] + \"/point_cloud/iteration_15000/point_cloud.ply\"\n",
        "\n",
        "    return (train_df[\"combined\"].tolist(), train_df[\"col1\"].tolist())\n",
        "\n",
        "def modify_paths_test():\n",
        "    delimiter = '/'\n",
        "    test_txt = data_path + \"/data/test.txt\"\n",
        "\n",
        "    #Read test file and split into two columns\n",
        "    test_df = pd.read_csv(test_txt, delimiter=delimiter, header=None, names=[\"col1\", \"col2\"])\n",
        "\n",
        "    #Construct paths to point cloud files\n",
        "    test_df[\"combined\"] = dataset_path + \"/\" + test_df[\"col1\"] + \"/\" + test_df[\"col2\"] + \"/point_cloud/iteration_15000/point_cloud.ply\"\n",
        "\n",
        "    return (test_df[\"combined\"].tolist() , test_df[\"col1\"].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iD4oDME8OOqK"
      },
      "outputs": [],
      "source": [
        "#Define all extra feature constants BEFORE get_args()\n",
        "FEATURE_ROTATION_QUAT = 'rotation_quat'\n",
        "FEATURE_ROTATION_MATRIX = 'rotation_matrix'\n",
        "FEATURE_SCALE = 'scale'\n",
        "FEATURE_COVARIANCE = 'covariance'\n",
        "FEATURE_OPACITY = 'opacity'\n",
        "FEATURE_COLOR = 'color'\n",
        "FEATURE_REST = 'rest'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gnxCP-D7CWCD",
        "outputId": "8068d34d-5180-42c6-83d0-575fc1372e3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "both train and testing paths are the same lenght as the lables\n"
          ]
        }
      ],
      "source": [
        "#Sanity test making sure the labels and paths have the same lenght meaning theres a l\n",
        "train_paths, trainlabels = modify_paths_train()\n",
        "test_paths, testlabels = modify_paths_test()\n",
        "testlabels\n",
        "\n",
        "\n",
        "if len(train_paths) == len(trainlabels) and len(test_paths) == len(testlabels):\n",
        "  print(\"both train and testing paths are the same lenght as the lables\")\n",
        "else:\n",
        "    print(\"PATHS ARE NOT SAME LENGHT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Setup and Configuration"
      ],
      "metadata": {
        "id": "ykRx--GQCrUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Random Sampling\n",
        "original_iter = sampler.RandomSampler.__iter__\n",
        "def patched_iter(self):\n",
        "    n = len(self.data_source)\n",
        "    if self.generator is None:\n",
        "        generator = torch.Generator()\n",
        "        generator.manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n",
        "    else:\n",
        "        generator = self.generator\n",
        "\n",
        "    if self.replacement:\n",
        "        for _ in range(self.num_samples // n):\n",
        "            for idx in torch.randint(0, n, size=(n,), generator=generator).tolist():\n",
        "                yield idx\n",
        "        for idx in torch.randint(0, n, size=(self.num_samples % n,), generator=generator).tolist():\n",
        "            yield idx\n",
        "    else:\n",
        "\n",
        "        for idx in torch.randperm(n, generator=generator).tolist():\n",
        "            yield idx\n",
        "\n",
        "sampler.RandomSampler.__iter__ = patched_iter\n",
        "\n",
        "# Function to save point clouds with class IDs\n",
        "def save_classified_pointcloud(points, class_ids, filename):\n",
        "    \"\"\"Save point cloud with class ID for each point.\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"ply\\n\")\n",
        "        f.write(\"format ascii 1.0\\n\")\n",
        "        f.write(f\"element vertex {len(points)}\\n\")\n",
        "        f.write(\"property float x\\n\")\n",
        "        f.write(\"property float y\\n\")\n",
        "        f.write(\"property float z\\n\")\n",
        "        f.write(\"property int class_id\\n\")\n",
        "        f.write(\"end_header\\n\")\n",
        "\n",
        "        for i in range(len(points)):\n",
        "            x, y, z = points[i]\n",
        "            class_id = int(class_ids[i])\n",
        "            f.write(f\"{x} {y} {z} {class_id}\\n\")\n",
        "\n",
        "# Read class names from train.txt/test.txt first\n",
        "def get_classes():\n",
        "    delimiter = '/'\n",
        "    # Read test file and extract class names\n",
        "    categories = pd.read_csv(test_txt, delimiter=delimiter, header=None, names=[\"col1\", \"col2\"])\n",
        "    categories_set = set(categories[\"col1\"])  # Get unique class names\n",
        "    sorted_classes = sorted(categories_set)  # Sort for consistency\n",
        "\n",
        "    print(f\"Sorted Categories from Dataset: {sorted_classes}\")\n",
        "    return sorted_classes\n",
        "\n",
        "# Set up class names and mappings\n",
        "CLASSES = get_classes()\n",
        "class2label = {cls: i for i, cls in enumerate(CLASSES)}\n",
        "CLASS2LABEL = class2label\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "print(f\"Class to label mapping: {CLASS2LABEL}\")\n",
        "\n",
        "seg_classes = class2label\n",
        "seg_label_to_cat = {}\n",
        "for i, cat in enumerate(seg_classes.keys()):\n",
        "    seg_label_to_cat[i] = cat\n",
        "\n",
        "def inplace_relu(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('ReLU') != -1:\n",
        "        m.inplace = True\n",
        "\n",
        "# Define ExtraFeature class if not imported from elsewhere\n",
        "class ExtraFeature:\n",
        "    @staticmethod\n",
        "    def feature_by_name(name):\n",
        "        # Simple placeholder implementation\n",
        "        return name\n",
        "\n",
        "\n",
        "class TrainEnv:\n",
        "    def log_string(self, str):\n",
        "        self.logger.info(str)\n",
        "        print(str)\n",
        "\n",
        "def create_environment(args, train_paths, test_paths, train_labels, test_labels):\n",
        "    env = TrainEnv()\n",
        "\n",
        "    '''HYPER PARAMETER'''\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "\n",
        "    '''CREATE DIR'''\n",
        "    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
        "    experiment_dir = Path('./log/')\n",
        "    experiment_dir.mkdir(exist_ok=True)\n",
        "    experiment_dir = experiment_dir.joinpath('sem_seg')\n",
        "    experiment_dir.mkdir(exist_ok=True)\n",
        "    if args.log_dir is None:\n",
        "        experiment_dir = experiment_dir.joinpath(timestr)\n",
        "    else:\n",
        "        experiment_dir = experiment_dir.joinpath(args.log_dir)\n",
        "    experiment_dir.mkdir(exist_ok=True)\n",
        "    env.checkpoints_dir = experiment_dir.joinpath('checkpoints/')\n",
        "    env.checkpoints_dir.mkdir(exist_ok=True)\n",
        "    log_dir = experiment_dir.joinpath('logs/')\n",
        "    log_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    '''LOG'''\n",
        "    env.logger = logging.getLogger(\"Model\")\n",
        "    env.logger.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n",
        "    file_handler.setLevel(logging.INFO)\n",
        "    file_handler.setFormatter(formatter)\n",
        "    env.logger.addHandler(file_handler)\n",
        "    env.log_string('PARAMETER ...')\n",
        "    env.log_string(args)\n",
        "\n",
        "    env.writer = SummaryWriter(log_dir=str(experiment_dir.joinpath('tensorboard')))\n",
        "\n",
        "    num_point = args.npoint\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    sampling = args.sampling\n",
        "\n",
        "    print(\"start loading training data ...\")\n",
        "    if args.dataset_type == '3DGS':\n",
        "        from datasets.composed_3dgs_dataset import Composed3DGSDataset\n",
        "\n",
        "        env.train_dataset = Composed3DGSDataset(\n",
        "            model_paths=train_paths,\n",
        "            class2label=CLASS2LABEL,  # Using class2label for label mapping\n",
        "            sampling=sampling,\n",
        "            num_point=num_point,\n",
        "            extra_features=args.extra_features  # Pass the `ExtraFeature` objects here\n",
        "        )\n",
        "    elif args.dataset_type == 'SampledMesh':\n",
        "        from datasets.composed_mesh_dataset import ComposedMeshDataset\n",
        "\n",
        "        env.train_dataset = ComposedMeshDataset(model_paths=train_paths, class2label=CLASS2LABEL, num_point=num_point)\n",
        "\n",
        "    print(\"start loading test data ...\")\n",
        "    if args.dataset_type == '3DGS':\n",
        "        env.test_dataset = Composed3DGSDataset(\n",
        "            model_paths=test_paths,\n",
        "            class2label=CLASS2LABEL,  # Using class2label for label mapping\n",
        "            sampling=sampling,\n",
        "            num_point=num_point,\n",
        "            extra_features=args.extra_features  # Pass the `ExtraFeature` objects here\n",
        "        )\n",
        "    elif args.dataset_type == 'SampledMesh':\n",
        "        env.test_dataset = ComposedMeshDataset(model_paths=test_paths, class2label=CLASS2LABEL, num_point=num_point)\n",
        "\n",
        "    def custom_worker_init_fn(worker_id):\n",
        "        # Use PyTorch's random number generator instead of NumPy's\n",
        "        worker_seed = torch.initial_seed() % 2**32\n",
        "        torch.manual_seed(worker_seed)\n",
        "        # Only set NumPy seed if it's safe to do so\n",
        "        try:\n",
        "            np.random.seed(worker_seed)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    env.trainDataLoader = torch.utils.data.DataLoader(\n",
        "        env.train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=False,  # Disabled pin_memory\n",
        "        drop_last=True,\n",
        "        num_workers=4,\n",
        "        worker_init_fn=custom_worker_init_fn\n",
        "    )\n",
        "\n",
        "    env.testDataLoader = torch.utils.data.DataLoader(\n",
        "        env.test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        pin_memory=False,  # Disabled pin_memory\n",
        "        drop_last=True,\n",
        "        num_workers=4,\n",
        "        worker_init_fn=custom_worker_init_fn\n",
        "    )\n",
        "\n",
        "    env.weights = torch.Tensor(env.train_dataset.label_weights).cuda()\n",
        "\n",
        "    env.log_string(\"The number of training data is: %d\" % len(env.train_dataset))\n",
        "    env.log_string(\"The number of test data is: %d\" % len(env.test_dataset))\n",
        "\n",
        "    '''MODEL LOADING'''\n",
        "    MODEL = importlib.import_module(args.model)\n",
        "\n",
        "    env.classifier = MODEL.get_model(NUM_CLASSES, env.train_dataset.get_channels_count).cuda()\n",
        "    env.classifier.apply(inplace_relu)\n",
        "    env.criterion = MODEL.get_loss().cuda()\n",
        "\n",
        "    def weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv2d') != -1:\n",
        "            torch.nn.init.xavier_normal_(m.weight.data)\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('Linear') != -1:\n",
        "            torch.nn.init.xavier_normal_(m.weight.data)\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(str(experiment_dir) + '/checkpoints/model.pth')\n",
        "        env.start_epoch = checkpoint['epoch'] + 1\n",
        "        env.classifier.load_state_dict(checkpoint['model_state_dict'])\n",
        "        env.log_string('Use pretrain model')\n",
        "    except:\n",
        "        env.log_string('No existing model, starting from scratch...')\n",
        "        env.start_epoch = 0\n",
        "        env.classifier = env.classifier.apply(weights_init)\n",
        "\n",
        "    if args.optimizer == 'Adam':\n",
        "        env.optimizer = torch.optim.Adam(\n",
        "            env.classifier.parameters(),\n",
        "            lr=args.learning_rate,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-08,\n",
        "            weight_decay=args.weight_decay_rate\n",
        "        )\n",
        "    else:\n",
        "        env.optimizer = torch.optim.SGD(env.classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n",
        "\n",
        "    return env\n",
        "\n",
        "\n",
        "def close_environment(env):\n",
        "    env.writer.close()\n",
        "\n",
        "    handlers = env.logger.handlers[:]\n",
        "    for handler in handlers:\n",
        "        env.logger.removeHandler(handler)\n",
        "        handler.close()\n",
        "        #Show classes"
      ],
      "metadata": {
        "id": "EBbrnu4RCC1n",
        "outputId": "1b5bea3c-fa54-46bd-9e1e-45a8a003231b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted Categories from Dataset: ['bathtub', 'bed', 'chair', 'desk', 'dresser', 'monitor', 'night_stand', 'sofa', 'table', 'toilet']\n",
            "Class to label mapping: {'bathtub': 0, 'bed': 1, 'chair': 2, 'desk': 3, 'dresser': 4, 'monitor': 5, 'night_stand': 6, 'sofa': 7, 'table': 8, 'toilet': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PointNet segmentation code I modified, with help from Claude AI\n",
        "I took the original PointNet segmentation code and made several improvements:\n",
        "- Streamlined how variables are declared for better organization\n",
        "- Added functionality to save point cloud visualization as PLY files\n",
        "- Created the EvalResults class for cleaner metric tracking\n",
        "\n",
        "Claude helped me troubleshoot the PLY file saving implementation and\n",
        "suggested refinements to the visualization approach. The core structure\n",
        "is from PointNet, but my modifications make it more usable for my needs."
      ],
      "metadata": {
        "id": "LUOkdBJX-uHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Declare your aguments here"
      ],
      "metadata": {
        "id": "uWerDt93EBKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change your arguments here\n",
        "def get_args():\n",
        "    args = argparse.Namespace()\n",
        "\n",
        "    args.data_path = \"/content/drive/MyDrive/3dgs/datasets\" #Change to your path\n",
        "    args.model = 'pointnet2_sem_seg'\n",
        "    args.dataset_type = '3DGS'\n",
        "    args.batch_size = 8\n",
        "    args.epoch = 200\n",
        "    args.learning_rate = 0.003\n",
        "    args.gpu = '0'\n",
        "    args.optimizer = 'Adam'\n",
        "    args.log_dir = 'epochs250_learningrate_003_bs32_v027_001_4096' #rename each time overwise the model will treat the previous version as a checkpoint\n",
        "    args.weight_decay_rate = 1e-4\n",
        "    args.npoint = 4096\n",
        "    args.lr_step_size = 5  # how often it updates\n",
        "    args.lr_decay = 0.95\n",
        "    args.eval_after_epoch = True\n",
        "    args.sampling = 'uniform'\n",
        "\n",
        "    # Test mode settings this is just to see if it will train and test (not good results)\n",
        "    args.test_mode = False #I added this so you can test a smaller sample size to see if the labeling runs and goes onto the next section\n",
        "    args.max_test_samples = 200\n",
        "    # Predefined extra features\n",
        "    predefined_extra_features = [\n",
        "        'rotation_quat',\n",
        "        'scale',\n",
        "        'opacity',\n",
        "    ]\n",
        "\n",
        "    # Convert to ExtraFeature objects\n",
        "    from data_utils.extra_feature import ExtraFeature\n",
        "    args.extra_features = [ExtraFeature.feature_by_name(feature) for feature in predefined_extra_features] if predefined_extra_features else None\n",
        "    return args\n"
      ],
      "metadata": {
        "id": "59Zi6VSHD9FI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation\n"
      ],
      "metadata": {
        "id": "80U2-d_FCmAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, args):\n",
        "    def bn_momentum_adjust(m, momentum):\n",
        "        if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n",
        "            m.momentum = momentum\n",
        "\n",
        "    # Set some constants for the learning scheduler\n",
        "    LEARNING_RATE_CLIP = 1e-5\n",
        "    MOMENTUM_ORIGINAL = 0.1\n",
        "    MOMENTUM_DECCAY = 0.5\n",
        "    MOMENTUM_DECCAY_STEP = args.lr_step_size\n",
        "\n",
        "    global_epoch = 0\n",
        "    best_iou = 0\n",
        "\n",
        "    #Main training loop\n",
        "    for epoch in range(env.start_epoch, args.epoch):\n",
        "        env.log_string('**** Epoch %d (%d/%s) ****' % (global_epoch + 1, epoch + 1, args.epoch))\n",
        "\n",
        "        # Decay learning rate over time\n",
        "        lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.lr_step_size)), LEARNING_RATE_CLIP)\n",
        "        env.log_string('Learning rate:%f' % lr)\n",
        "        for param_group in env.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        #Update batch norm momentum - helps stabilize training\n",
        "        momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n",
        "        if momentum < 0.01:\n",
        "            momentum = 0.01\n",
        "        print('BN momentum updated to: %f' % momentum)\n",
        "        env.classifier = env.classifier.apply(lambda x: bn_momentum_adjust(x, momentum))\n",
        "\n",
        "        env.writer.add_scalar('LR', lr, epoch)\n",
        "\n",
        "        # Reset metrics for this epoch\n",
        "        num_batches = len(env.trainDataLoader)\n",
        "        total_correct = 0\n",
        "        total_seen = 0\n",
        "        loss_sum = 0\n",
        "        env.classifier = env.classifier.train()\n",
        "\n",
        "        # Process each batch\n",
        "        for i, (points, target) in tqdm(enumerate(env.trainDataLoader), total=len(env.trainDataLoader), smoothing=0.9):\n",
        "            env.optimizer.zero_grad()\n",
        "\n",
        "            # Prep the point cloud data\n",
        "            points = points.data.numpy()\n",
        "            points = torch.Tensor(points)\n",
        "            points, target = points.float().cuda(), target.long().cuda()\n",
        "            points = points.transpose(2, 1)  # PointNet needs this format\n",
        "\n",
        "            # Forward pass through model\n",
        "            seg_pred, trans_feat = env.classifier(points)\n",
        "            seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
        "\n",
        "            batch_label = target.view(-1, 1)[:, 0].cpu().data.numpy()\n",
        "            target = target.view(-1, 1)[:, 0]\n",
        "\n",
        "            # Calculate loss and update weights\n",
        "            loss = env.criterion(seg_pred, target, trans_feat, env.weights)\n",
        "            loss.backward()\n",
        "            env.optimizer.step()\n",
        "\n",
        "            # Track how we're doing\n",
        "            pred_choice = seg_pred.cpu().data.max(1)[1].numpy()\n",
        "            correct = np.sum(pred_choice == batch_label)\n",
        "            total_correct += correct\n",
        "            total_seen += (args.batch_size * args.npoint)\n",
        "            loss_sum += loss\n",
        "\n",
        "        # Log training results\n",
        "        training_loss = loss_sum / num_batches\n",
        "        training_accuracy = total_correct / float(total_seen)\n",
        "\n",
        "        env.log_string('Training mean loss: %f' % training_loss)\n",
        "        env.log_string('Training accuracy: %f' % training_accuracy)\n",
        "        env.writer.add_scalar('Train loss', training_loss, epoch)\n",
        "        env.writer.add_scalar('Train accuracy', training_accuracy, epoch)\n",
        "\n",
        "        # Save model checkpoints regularly\n",
        "        if epoch % 5 == 0 or epoch == args.epoch - 1:\n",
        "            env.log_string('Save model...')\n",
        "            savepath = str(env.checkpoints_dir) + '/model.pth'\n",
        "            env.log_string('Saving at %s' % savepath)\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': env.classifier.state_dict(),\n",
        "                'optimizer_state_dict': env.optimizer.state_dict(),\n",
        "            }\n",
        "            torch.save(state, savepath)\n",
        "            env.log_string('Saving model....')\n",
        "\n",
        "        # Evaluation phase\n",
        "        if args.eval_after_epoch:\n",
        "            env.log_string('---- EPOCH %03d EVALUATION ----' % (global_epoch + 1))\n",
        "            eval_results = evaluate(env, args, epoch=epoch)\n",
        "\n",
        "            #Log Progress\n",
        "            env.log_string('eval point avg class IoU: %f' % eval_results.mIoU)\n",
        "            env.log_string('eval point avg class acc: %f' % (\n",
        "                eval_results.mean_class_accuracy))\n",
        "\n",
        "            # Show per-class results\n",
        "            iou_per_class_str = '------- IoU --------\\n'\n",
        "            for l in range(NUM_CLASSES):\n",
        "                iou_per_class_str += 'class %s weight: %.3f, IoU: %.3f \\n' % (\n",
        "                    seg_label_to_cat[l] + ' ' * (14 - len(seg_label_to_cat[l])),\n",
        "                    eval_results.labelweights[l - 1],\n",
        "                    eval_results.class_mIoU[l]\n",
        "                )\n",
        "\n",
        "            env.log_string(iou_per_class_str)\n",
        "            env.log_string('Eval mean loss: %f' % eval_results.loss)\n",
        "            env.log_string('Eval accuracy: %f' % eval_results.accuracy)\n",
        "\n",
        "            env.writer.add_scalar('Eval loss', eval_results.loss, epoch)\n",
        "            env.writer.add_scalar('Eval accuracy', eval_results.accuracy, epoch)\n",
        "            env.writer.add_scalar('Eval mIoU', eval_results.mIoU, epoch)\n",
        "\n",
        "            if eval_results.mIoU >= best_iou:\n",
        "                best_iou = eval_results.mIoU\n",
        "                env.log_string('Save model...')\n",
        "                savepath = str(env.checkpoints_dir) + '/best_model.pth'\n",
        "                env.log_string('Saving at %s' % savepath)\n",
        "                state = {\n",
        "                    'epoch': epoch,\n",
        "                    'class_avg_iou': eval_results.mIoU,\n",
        "                    'model_state_dict': env.classifier.state_dict(),\n",
        "                    'optimizer_state_dict': env.optimizer.state_dict(),\n",
        "                }\n",
        "                torch.save(state, savepath)\n",
        "                env.log_string('Saving model....')\n",
        "            env.log_string('Best mIoU: %f' % best_iou)\n",
        "\n",
        "        env.writer.flush()\n",
        "        global_epoch += 1\n",
        "\n",
        "\n",
        "# Container for all our evaluation metrics\n",
        "class EvalResults:\n",
        "    def __init__(self, mIoU, loss, accuracy, labelweights, mean_class_accuracy, class_mIoU):\n",
        "        self.mIoU = mIoU\n",
        "        self.loss = loss\n",
        "        self.accuracy = accuracy\n",
        "        self.labelweights = labelweights\n",
        "        self.mean_class_accuracy = mean_class_accuracy\n",
        "        self.class_mIoU = class_mIoU\n",
        "\n",
        "\n",
        "def evaluate(env, args, epoch=None):\n",
        "    with torch.no_grad():\n",
        "        # Setup tracking variables\n",
        "        num_batches = len(env.testDataLoader)\n",
        "        total_correct = 0\n",
        "        total_seen = 0\n",
        "        loss_sum = 0\n",
        "        labelweights = np.zeros(NUM_CLASSES)\n",
        "        total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
        "        total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
        "        total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n",
        "        env.classifier = env.classifier.eval()  # Switch to eval mode\n",
        "\n",
        "        # Save visualization files every 50 epochs\n",
        "        save_visualization = (epoch is not None) and (epoch % 50 == 0 or epoch == args.epoch - 1)\n",
        "\n",
        "        # Need these lists for visualization\n",
        "        if save_visualization:\n",
        "            all_points = []\n",
        "            all_predictions = []\n",
        "            all_ground_truth = []\n",
        "\n",
        "        # Process test batches\n",
        "        for i, (points, target) in tqdm(enumerate(env.testDataLoader), total=len(env.testDataLoader), smoothing=0.9):\n",
        "            original_points = points.data.numpy()\n",
        "\n",
        "            points = torch.Tensor(original_points)\n",
        "            points, target = points.float().cuda(), target.long().cuda()\n",
        "            points = points.transpose(2, 1)\n",
        "\n",
        "            # Get model predictions\n",
        "            seg_pred, trans_feat = env.classifier(points)\n",
        "            pred_val = seg_pred.contiguous().cpu().data.numpy()\n",
        "            seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
        "\n",
        "            batch_label = target.cpu().data.numpy()\n",
        "            target = target.view(-1, 1)[:, 0]\n",
        "            loss = env.criterion(seg_pred, target, trans_feat, env.weights)\n",
        "            loss_sum += loss\n",
        "            pred_val = np.argmax(pred_val, 2)  # Get class predictions\n",
        "            correct = np.sum((pred_val == batch_label))\n",
        "            total_correct += correct\n",
        "            total_seen += (args.batch_size * args.npoint)\n",
        "            tmp, _ = np.histogram(batch_label, range(NUM_CLASSES + 1))\n",
        "            labelweights += tmp\n",
        "\n",
        "            # Per-class metrics\n",
        "            for l in range(NUM_CLASSES):\n",
        "                total_seen_class[l] += np.sum((batch_label == l))\n",
        "                total_correct_class[l] += np.sum((pred_val == l) & (batch_label == l))\n",
        "                total_iou_deno_class[l] += np.sum(((pred_val == l) | (batch_label == l)))\n",
        "\n",
        "            # Collect visualization data if needed\n",
        "            if save_visualization:\n",
        "                for b in range(original_points.shape[0]):\n",
        "                    # Just grab XYZ coordinates\n",
        "                    batch_points = original_points[b, :, :3]\n",
        "                    batch_pred = pred_val[b]\n",
        "                    batch_gt = batch_label[b]\n",
        "\n",
        "                    all_points.append(batch_points)\n",
        "                    all_predictions.append(batch_pred)\n",
        "                    all_ground_truth.append(batch_gt)\n",
        "\n",
        "        # Save colorized point clouds for visualization\n",
        "        if save_visualization:\n",
        "            vis_dir = os.path.join(str(env.checkpoints_dir), 'visualizations')\n",
        "            os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "            # Combine all the collected data\n",
        "            full_points = np.vstack(all_points)\n",
        "            full_predictions = np.concatenate(all_predictions)\n",
        "            full_ground_truth = np.concatenate(all_ground_truth)\n",
        "\n",
        "            # Save as PLY files that can be viewed in 3D software\n",
        "            pred_filename = os.path.join(vis_dir, f'epoch_{epoch}_full_prediction.ply')\n",
        "            save_classified_pointcloud(full_points, full_predictions, pred_filename)\n",
        "\n",
        "            gt_filename = os.path.join(vis_dir, f'epoch_{epoch}_full_groundtruth.ply')\n",
        "            save_classified_pointcloud(full_points, full_ground_truth, gt_filename)\n",
        "\n",
        "        # Calculate final metrics\n",
        "        labelweights = labelweights.astype(np.float32) / np.sum(labelweights.astype(np.float32))\n",
        "        mIoU = np.mean(np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=float) + 1e-6))\n",
        "        eval_loss = loss_sum / float(num_batches)\n",
        "        eval_accuracy = total_correct / float(total_seen)\n",
        "        mean_class_accuracy = np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=float) + 1e-6))\n",
        "        class_mIoU = [total_correct_class[l] / float(total_iou_deno_class[l]) for l in range(NUM_CLASSES)]\n",
        "\n",
        "        return EvalResults(mIoU, eval_loss, eval_accuracy, labelweights, mean_class_accuracy, class_mIoU)\n",
        "\n",
        "def main():\n",
        "    # Get the config settings\n",
        "    args = get_args()\n",
        "\n",
        "    # Load dataset paths and their labels\n",
        "    train_paths, trainlabels = modify_paths_train()\n",
        "    test_paths, testlabels = modify_paths_test()\n",
        "\n",
        "    # Use fewer samples in test mode\n",
        "    if args.test_mode:\n",
        "        print(f\"Test mode enabled - using only {args.max_test_samples} samples\")\n",
        "        train_paths = train_paths[:args.max_test_samples]\n",
        "        trainlabels = trainlabels[:args.max_test_samples]\n",
        "        test_paths = test_paths[:args.max_test_samples]\n",
        "        testlabels = testlabels[:args.max_test_samples]\n",
        "\n",
        "    # Make sure our data is valid\n",
        "    if len(train_paths) == len(trainlabels) and len(test_paths) == len(testlabels):\n",
        "        # Setup and run training\n",
        "        env = create_environment(args, train_paths, test_paths, trainlabels, testlabels)\n",
        "        train(env, args)\n",
        "        close_environment(env)\n",
        "    else:\n",
        "        # Something's wrong with the data\n",
        "        raise Exception(\"The list of labels for the train and test data do not match lengths.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "FZ8pcQlfCi1I",
        "outputId": "769bc8ca-a27d-432e-8d22-29d8437cf885",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Model:PARAMETER ...\n",
            "INFO:Model:Namespace(data_path='/content/drive/MyDrive/3dgs/datasets', model='pointnet2_sem_seg', dataset_type='3DGS', batch_size=8, epoch=200, learning_rate=0.003, gpu='0', optimizer='Adam', log_dir='epochs250_learningrate_003_bs32_v027_001_4096', weight_decay_rate=0.0001, npoint=4096, lr_step_size=5, lr_decay=0.95, eval_after_epoch=True, sampling='uniform', test_mode=False, max_test_samples=200, extra_features=[<data_utils.extra_feature.ExtraFeature object at 0x791d469e1790>, <data_utils.extra_feature.ExtraFeature object at 0x791d4734d1d0>, <data_utils.extra_feature.ExtraFeature object at 0x791d4734f5d0>])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PARAMETER ...\n",
            "Namespace(data_path='/content/drive/MyDrive/3dgs/datasets', model='pointnet2_sem_seg', dataset_type='3DGS', batch_size=8, epoch=200, learning_rate=0.003, gpu='0', optimizer='Adam', log_dir='epochs250_learningrate_003_bs32_v027_001_4096', weight_decay_rate=0.0001, npoint=4096, lr_step_size=5, lr_decay=0.95, eval_after_epoch=True, sampling='uniform', test_mode=False, max_test_samples=200, extra_features=[<data_utils.extra_feature.ExtraFeature object at 0x791d469e1790>, <data_utils.extra_feature.ExtraFeature object at 0x791d4734d1d0>, <data_utils.extra_feature.ExtraFeature object at 0x791d4734f5d0>])\n",
            "start loading training data ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3991 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0001/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/3991 [00:01<1:36:43,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0002/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 2/3991 [00:02<1:24:47,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0003/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 3/3991 [00:04<1:31:48,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0004/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 4/3991 [00:05<1:37:39,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0005/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 5/3991 [00:06<1:19:01,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0006/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 6/3991 [00:07<1:09:43,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0007/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 7/3991 [00:08<1:14:38,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0008/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 8/3991 [00:09<1:15:00,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0009/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 9/3991 [00:10<1:03:59,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0010/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 10/3991 [00:10<1:00:10,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0011/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 11/3991 [00:11<56:16,  1.18it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0012/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 12/3991 [00:13<1:07:29,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0013/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 13/3991 [00:13<1:04:00,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0014/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 14/3991 [00:14<1:03:50,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0015/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 15/3991 [00:15<1:00:00,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0016/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 16/3991 [00:16<1:02:38,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0017/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 17/3991 [00:17<1:07:52,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0018/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 18/3991 [00:18<1:07:42,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0019/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 19/3991 [00:20<1:13:01,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0020/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 20/3991 [00:21<1:16:04,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0021/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 21/3991 [00:22<1:10:28,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0022/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 22/3991 [00:23<1:08:30,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0023/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 23/3991 [00:24<1:11:27,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0024/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 24/3991 [00:25<1:15:27,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0025/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 25/3991 [00:27<1:18:31,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0026/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 26/3991 [00:28<1:14:23,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0027/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 27/3991 [00:28<1:08:31,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0028/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 28/3991 [00:29<1:06:25,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0029/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 29/3991 [00:31<1:11:02,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0030/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 30/3991 [00:31<1:04:09,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0031/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 31/3991 [00:32<1:04:56,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0032/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 32/3991 [00:33<1:00:16,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0033/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 33/3991 [00:34<1:03:53,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0034/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 34/3991 [00:35<1:05:18,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0035/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 35/3991 [00:36<1:00:41,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0036/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 36/3991 [00:37<57:14,  1.15it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0037/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 37/3991 [00:38<57:26,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0038/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 38/3991 [00:39<1:09:26,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0039/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 39/3991 [00:40<1:01:55,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0040/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 40/3991 [00:41<1:05:42,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0041/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 41/3991 [00:42<1:06:39,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0042/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 42/3991 [00:43<1:06:00,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0043/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 43/3991 [00:44<1:07:34,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0044/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 44/3991 [00:46<1:23:13,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0045/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 45/3991 [00:47<1:16:58,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0046/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 46/3991 [00:48<1:10:14,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0047/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 47/3991 [00:50<1:28:22,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0048/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 48/3991 [00:50<1:19:34,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0049/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 49/3991 [00:52<1:16:35,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0050/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|         | 50/3991 [00:53<1:13:32,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0051/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|         | 51/3991 [00:53<1:04:34,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0052/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|         | 52/3991 [00:54<1:08:11,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0053/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|         | 53/3991 [00:55<1:04:27,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0054/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|         | 54/3991 [00:56<1:08:15,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0055/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|         | 55/3991 [00:58<1:15:12,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0056/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|         | 56/3991 [00:59<1:11:42,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0057/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|         | 57/3991 [01:00<1:14:12,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0058/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|         | 58/3991 [01:01<1:08:21,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0059/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|         | 59/3991 [01:02<1:14:57,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0060/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 60/3991 [01:03<1:11:53,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0061/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 61/3991 [01:04<1:04:37,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0062/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 62/3991 [01:05<1:05:47,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0063/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 63/3991 [01:06<1:06:28,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0064/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 64/3991 [01:07<1:12:02,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0065/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 65/3991 [01:08<1:13:12,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0066/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 66/3991 [01:10<1:15:46,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0067/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 67/3991 [01:11<1:15:07,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0068/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 68/3991 [01:12<1:07:33,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0069/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 69/3991 [01:12<1:02:10,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0070/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 70/3991 [01:13<57:26,  1.14it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0071/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 71/3991 [01:14<56:43,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0072/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 72/3991 [01:15<52:00,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0073/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 73/3991 [01:16<1:01:05,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0074/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 74/3991 [01:17<57:53,  1.13it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0075/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 75/3991 [01:18<1:02:42,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classname: bathtub, path: /content/drive/MyDrive/3dgs/datasets/bathtub/bathtub_0076/point_cloud/iteration_15000/point_cloud.ply\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1_nlGgzlxcE5SWaBOGG1inlpsHKJgancb",
      "authorship_tag": "ABX9TyMuqHYdoQ1Ij9qZ9+J2NQZF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}